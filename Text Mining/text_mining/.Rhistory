freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
source("./Rscript/createChoyJson_v2.R")
json_lda = createChoyJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA <- use this when it doesnt work
mds.method = canberraPCA
)
serVis(json_lda, open.browser = T) # Use this when you are using mac
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
## Add Space between words ##
parsedData = gsub(" ","  ",parsedData)
#Create Corpus
corp=VCorpus(VectorSource(parsedData))
#corp=VCorpus(VectorSource(parsedDataRe$parsedContent))
#Remove Puncutuation and Special Characters
corp = tm_map(corp, removePunctuation)
#Chnage to lower case letters
corp = tm_map(corp, tolower)
#Remove particular words
corp = tm_map(corp, removeWords, c("is", "the","or","and","for","that","this","more","so", "be"))
#manage synonyms
for (j in seq(corp))
{
corp[[j]] <- gsub("like", "love", corp[[j]])
corp[[j]] <- gsub("hate", "dislike", corp[[j]])
}
##################################################################
#change to text document form
corp = tm_map(corp, PlainTextDocument)
#create Document Term Matrix (DTM) - (length of the word is set as 2)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
dtm
#Remove one letter word
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
#Remove Sparse Terms
dtm = removeSparseTerms(dtm, as.numeric(0.997))
dtm
## When using LDA, control the size of DTM
#Find the value of Tf-Idf for each word
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
#Check the distribution using boxplot
boxplot(term_tfidf)
# Based on the value of Tf-Idf, reduce the size of dtm and create new_dtm
new_dtm = dtm[,term_tfidf >= 0.1]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
###########################################
#set up random seed, name of the analysis, and the number of cluster
name = "OatmealCookies" #give a name
SEED = 2017 #set up ramdom seed
k = 10 # set up the number of lcuster
#launch LDA
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)
lda_tm
#Save important words for each topic
term_topic = terms(lda_tm, 30)
#Print the important words for each topic through file
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
# Save the topic number of each file
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
#calculate the probability of  topic of each file
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
#Find the maximum probability value
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
#Find topic number for each file and extract the probablity value
doc_Prob_df$rown = doc_topic_df$rown
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","doc_topic","maxProb"))
#Extract the probability value of each topic number
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)
#Print the probability value of each topic word
posterior(lda_tm)$terms
# phi is the probability value that is included in the topics of each word.
phi = posterior(lda_tm)$terms %>% as.matrix
# theta is the probability value that is included in the topics of each file.
theta = posterior(lda_tm)$topics %>% as.matrix
# vocab is the list of all the words.
vocab = colnames(phi)
# Find the length of the file of each file.
doc_length = vector()
doc_topic_df=as.data.frame(doc_topic)
for( i in as.numeric(row.names(doc_topic_df))){
temp = corp[[i]]$content
doc_length = c(doc_length, nchar(temp[1]))
}
# Find the freqency of each word.
new_dtm_m = as.matrix(new_dtm)
freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
source("./Rscript/createChoyJson_v2.R")
json_lda = createChoyJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA <- use this when it doesnt work
mds.method = canberraPCA
)
serVis(json_lda, open.browser = T) # Use this when you are using mac
install.packages("tm") #Package for text mining
install.packages("slam")
install.packages("dplyr")
install.packages("readr") #Package to read file
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(NLP4kec)
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
install.packages("janeaustenr")
library(janeaustenr)
install.packages("dplyr")
install.packages("stringr")
library(dplyr)
library(stringr)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber =row_number(),
chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]",
ignocre_case = TRUE))) %>%
ungroup()
original_books
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber =row_number(),
chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]",
ignocre_case = TRUE))) %>%
ungroup()
original_books
corp = tm_map(corp, PlainTextDocument)
install.packages("tm") #Package for text mining
nstall.packages("tm") #Package for text mining
install.packages("slam")
install.packages("dplyr")
install.packages("readr") #Package to read file
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(NLP4kec)
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
corp = VCorpus(VectorSource(parsedData))
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, removeNumbers)
corp = tm_map(corp, tolower)
corp = tm_map(corp, removeWords, c("is", "the","or","and","for","that","this","more","so", "be"))
for (j in seq(corp))
{
corp[[j]] <- gsub("like", "love", corp[[j]])
corp[[j]] <- gsub("hate", "dislike", corp[[j]])
}
corp = tm_map(corp, PlainTextDocument)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
dtm
tdm = TermDocumentMatrix(corp, control=list(removeNumbers=TRUE, wordLengths=c(2,Inf)))
dtm = removeSparseTerms(dtm, as.numeric(0.98))
freq = colSums(as.matrix(dtm))
head(freq)
dtm_df = as.data.frame(as.matrix(dtm))
write_excel_csv(dtm_df, "./dtm.csv")
length(freq)
freq[head(order(-freq), 5)]
freq[head(order(freq), 10)]
findFreqTerms(dtm, lowfreq = 20, highfreq = 341)
wordDf = data.frame(word=names(freq), freq=freq)
library(ggplot2)
install.packages("extrafont")
library(extrafont)
loadfonts(device="postscript")
ggplot(wordDf, aes(x=word, y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
findFreqTerms(dtm, lowfreq = 20, highfreq = 341)
wordDf = data.frame(word=names(freq), freq=freq)
ggplot(wordDf, aes(x=word, y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
findFreqTerms(dtm, lowfreq = 20, highfreq = 341)
wordDf = data.frame(word=names(freq), freq=freq)
ggplot(wordDf, aes(x=word, y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
ggplot(head(wordDf,10), aes(x=word, y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
ggplot(head(arrange(wordDf,-freq),20), aes(x=reorder(word,-freq), y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
install.packages("wordcloud")
library(wordcloud)
pal = brewer.pal(n = 12, name = "Set2") #  n:number of colors you want to use, name:the set of color
wordcloud(wordDf$word # word
, wordDf$freq # frequency
, min.freq = 5 # minimum frequency
, colors = pal # palette information
, rot.per = 0.5 # word rotation degree
, random.order = F # decision of word appearance in random way (False or True)
, scale = c(3,1) # the front number should be large so the most freqeuntly appeared number can show up in greater size
, family="AppleGothic") # Macbook users need to set font
install.packages("treemap")
library(treemap)
treemap(wordDf # set data
,title = "Word Tree Map"
,index = c("word") # set variable that will go inside the box
,vSize = "freq"  # set box size
,fontfamily.labels = "AppleGothic" # Macbook users need to set font
,fontsize.labels = 12 # set font size
,palette=pal # palette information
,border.col = "white") # set border color
View(dtm_df)
findAssocs(dtm, terms = "cookie", corlimit = 0.2)  #insert your keyword here. For example I have put cookie here
dtm_m = as.matrix(dtm)
dtm_m
cor_term = cor(dtm_m)
cor_term
cor_ref = cor_term[,"cookie"]
cor_ref
dtmW = DocumentTermMatrix(corp, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
dtmW
colnames(dtmW) = trimws(colnames(dtmW))
dtmW = dtmW[,nchar(colnames(dtm)) > 1]
dtmW = removeSparseTerms(dtmW, as.numeric(0.96))
dtmW
findAssocs(dtmW, "cookie", 0.01)
install.packages(c("igraph", "network", "sna", "GGally"))
library(igraph)
library(network)
library(sna)
library(ggplot2)
library(GGally)
dtmW_m = as.matrix(dtmW)
cor_termW = cor(dtmW_m)
cor_termW[cor_termW < 0.25] = 0
net = network(cor_termW, directed = FALSE)
net
net %v% "mode" <- ifelse(betweenness(net) > quantile(betweenness(net), 0.9), "big", "small")
node_color = c("small" = "grey", "big" = "gold")
set.edge.value(net, "edgeSize", cor_termW * 2)
ggnet2(net # Network
,label=TRUE # Node label - true or false
,label.size = 3 # label font size
,color = "mode" # node color standard
,palette = node_color # node color
,size = "degree" # the size of the node is differed based on the degree cetrality value
,edge.size = "edgeSize" #
,family="AppleGothic") # mac users need this
keyword = c("grain","baked","sample")  #This is just an example
sub_cor_term = cor_termW[,keyword]
head(sub_cor_term)
sub_cor_term = sub_cor_term[!(rownames(sub_cor_term) %in% keyword),]
head(sub_cor_term)
sub_cor_term = sub_cor_term[rowSums(sub_cor_term)>0,]
head(sub_cor_term)
net2 = network(sub_cor_term, directed = FALSE, matrix.type="bipartite")
ggnet2(net2
,label=TRUE
,label.size = 3
,edge.size = sub_cor_term[sub_cor_term>0] * 2
,size = degree(net2)
,family="AppleGothic")
install.packages("tsne")
install.packages("devtools")
library(devtools)   #Rtools needs to be downloaded separately in Windows
install_github("bmschmidt/wordVectors")
library(wordVectors)
library(tsne)
library(readr)
system("tctStart")
targetData = read_csv("/Users/kimnamyoun/TextConvert4TM/output/out_HomeApplication_cafe.csv")
write.table(targetData$parsedContent,file = "./trainTxt.txt", row.names = FALSE, col.names = FALSE, quote = F)
targetData = read_csv("./Rscript/Oatmeal_Cookies.xlsx")
write.table(targetData$parsedContent,file = "./trainTxt.txt", row.names = FALSE, col.names = FALSE, quote = F)
model = train_word2vec("./trainTxt.txt", output_file = "w2vModel.bin",
threads=3, vectors=100, force = T)
read.vectors("./w2vModel.bin")
nearest_to(model,model[["냉장고"]], 20)
nearest_to(model,model[[c("냉장고","양문")]], 20)
subVec = model[rownames(model)=="냉장고",] - model[rownames(model) == "디오스",] + model[rownames(model) == "트롬",]
nearest_to(model, subVec, 20)
library(extrafont)
par(family="AppleGothic")
plot(model)
targetData$parsedContent = tolower(targetData$parsedContent)
cosineSimilarity(model[["냉장고"]], model[["그룹"]])
dist(model[(row.names(model)=="냉장고" | row.names(model)=="그룹"),])
install.packages("topicmodels")
install.packages("LDAvis")
install.packages("servr")
library(topicmodels)
library(LDAvis)
library(servr)
library(readr)
library(tm)
library(slam)
library(dplyr)
library(NLP4kec)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
##################################################################
### 2. Text Handling - Call the file that consists the text    ###
##################################################################
#Launch Morpheme Analyzer (use Standford Core NLP )
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
##################################################################
### 3. Text Handling - Text Pre-processing                     ###
##################################################################
## Add Space between words ##
parsedData = gsub(" ","  ",parsedData)
#Create Corpus
corp=VCorpus(VectorSource(parsedData))
#corp=VCorpus(VectorSource(parsedDataRe$parsedContent))
#Remove Puncutuation and Special Characters
corp = tm_map(corp, removePunctuation)
#Chnage to lower case letters
corp = tm_map(corp, tolower)
#Remove particular words
corp = tm_map(corp, removeWords, c("is", "the","or","and","for","that","this","more","so", "be"))
#manage synonyms
for (j in seq(corp))
{
corp[[j]] <- gsub("like", "love", corp[[j]])
corp[[j]] <- gsub("hate", "dislike", corp[[j]])
}
#######################################################
#change to text document form
corp = tm_map(corp, PlainTextDocument)
#create Document Term Matrix (DTM) - (length of the word is set as 2)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
dtm
#Remove one letter word
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
#Remove Sparse Terms
dtm = removeSparseTerms(dtm, as.numeric(0.997))
dtm
## When using LDA, control the size of DTM
#Find the value of Tf-Idf for each word
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
#Check the distribution using boxplot
boxplot(term_tfidf)
# Based on the value of Tf-Idf, reduce the size of dtm and create new_dtm
new_dtm = dtm[,term_tfidf >= 0.1]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
############################################
## Running LDA
############################################
#set up random seed, name of the analysis, and the number of cluster
name = "OatmealCookies" #give a name
SEED = 2017 #set up ramdom seed
k = 10 # set up the number of lcuster
#launch LDA
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)
lda_tm
#Save important words for each topic
term_topic = terms(lda_tm, 30)
#Print the important words for each topic through file
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
# Save the topic number of each file
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
#calculate the probability of  topic of each file
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
#Find the maximum probability value
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
#Find topic number for each file and extract the probablity value
doc_Prob_df$rown = doc_topic_df$rown
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","doc_topic","maxProb"))
#Extract the probability value of each topic number
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)
#Print the probability value of each topic word
posterior(lda_tm)$terms
##################################################################
### 3. Data Visualization of Topic Clustering                  ###
##################################################################
# phi is the probability value that is included in the topics of each word.
phi = posterior(lda_tm)$terms %>% as.matrix
# theta is the probability value that is included in the topics of each file.
theta = posterior(lda_tm)$topics %>% as.matrix
# vocab is the list of all the words.
vocab = colnames(phi)
# Find the length of the file of each file.
doc_length = vector()
doc_topic_df=as.data.frame(doc_topic)
for( i in as.numeric(row.names(doc_topic_df))){
temp = corp[[i]]$content
doc_length = c(doc_length, nchar(temp[1]))
}
# Find the freqency of each word.
new_dtm_m = as.matrix(new_dtm)
freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
# We change the value above to a parameter value in order to create a data for visualization.
source("./Rscript/createChoyJson_v2.R")
json_lda = createChoyJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA <- use this when it doesnt work
mds.method = canberraPCA
)
serVis(json_lda, open.browser = T) # Use this when you are using mac
install.packages("tm") #Package for text mining
install.packages("slam")
install.packages("dplyr")
install.packages("readr") #Package to read file
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
#Launch Morpheme Analyzer (use Standford Core NLP )
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
#Launch Morpheme Analyzer (use Standford Core NLP )
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
library(NLP4kec)
install.packages("NLP4Kec")
library(NLP4kec)
install.packages("NLP4kec")
install.packages("rJava")
library(rJava)
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre8')
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
library(tm)
library(slam)
library(dplyr)
library(readr)
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
Sys.getenv("DYLD_FALLBACK_LIBRARY_PATH")
dyn.load("/Library/Frameworks/R.framework/Resources/lib:/Users/danielchoy/lib:/usr/local/lib:/usr/lib:::/lib:/Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home/jre/lib/server")
install.packages("rJava")
library(rJava)
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home")
dyn.load('/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/lib/server/libjvm.dylib')
library(rJava)
#Launch Morpheme Analyzer (use Standford Core NLP )
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
library(rJava)
install.packages("rJava")
library(NLP4kec)
library(NLP4kec)
load("/Users/danielchoy/Downloads/NLP4kec/Meta/package.rds")
install.packages("/Users/danielchoy/Downloads/NLP4kec_1.0.0.zip" , repos=NULL)
install.packages("/Users/danielchoy/Downloads/NLP4kec_1.0.0.zip", repos=NULL)
install.packages("/Users/danielchoy/Downloads/NLP4kec_1.0.0.zip")
?About
?version
R.version
install.packages("tidytext")
library(tidytext)
install.packages("glue")
library(tidyverse)
library(tidytext)
library(glue)
library(stringr)
R.version
library(NLP4kec)
install.packages("/Users/danielchoy/Downloads/NLP4kec_1.0.0.zip")
library(rJava)
install.packages("rJava")
install.packages("rJava")
library(rJava)
dyn.load('/Library/Java/JavaVirtualMachines/jdk-9.0.4.jdk/Contents/Home/lib/server/libjvm.dylib')
#Launch Morpheme Analyzer (use Standford Core NLP )
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
