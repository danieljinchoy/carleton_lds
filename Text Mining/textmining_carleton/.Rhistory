install.packages("tm") #Package for text mining
install.packages("slam")
install.packages("dplyr")
install.packages("readr") #Package to read file
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(NLP4kec)
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en")
parsedData
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "ko")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "ko")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en", korDicPath = "./dictionary.txt")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx", language = "en", korDicPath = "./dictionary.txt")
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx"
,language = "en"
,korDicPath = "./dictionary.txt")
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(NLP4kec)
parsedData = text_parser(path = "./Oatmeal_Cookies.xlsx"
,language = "en"
,korDicPath = "./dictionary.txt")
install.packages("tm") #Package for text mining
install.packages("slam")
install.packages("dplyr")
install.packages("readr") #Package to read file
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(NLP4kec)
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
corp = VCorpus(VectorSource(parsedData))
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, removeNumbers)
corp = tm_map(corp, tolower)
corp = tm_map(corp, removeWords, c("is", "the","or","and","for","that","this","more","so"))
for (j in seq(corp))
{
corp[[j]] <- gsub("like", "love", corp[[j]])
corp[[j]] <- gsub("hate", "dislike", corp[[j]])
}
corp = tm_map(corp, PlainTextDocument)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
tdm = TermDocumentMatrix(corp, control=list(removeNumbers=TRUE, wordLengths=c(2,Inf)))
dtm = removeSparseTerms(dtm, as.numeric(0.98))
freq = colSums(as.matrix(dtm))
head(freq)
dtm_df = as.data.frame(as.matrix(dtm))
write_excel_csv(dtm_df, "./dtm.csv")
length(freq)
freq[head(order(-freq), 5)]
freq[head(order(freq), 10)]
findFreqTerms(dtm, lowfreq = 20, highfreq = 341)
wordDf = data.frame(word=names(freq), freq=freq)
library(ggplot2)
install.packages("extrafont")
library(extrafont)
loadfonts(device="postscript")
ggplot(wordDf, aes(x=word, y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
ggplot(head(wordDf,10), aes(x=word, y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
ggplot(head(arrange(wordDf,-freq),20), aes(x=reorder(word,-freq), y=freq)) + geom_bar(stat = "identity") + theme(axis.text.x = element_text(family = "AppleGothic"))
install.packages("wordcloud")
library(wordcloud)
pal = brewer.pal(n = 12, name = "Set2") #  n:number of colors you want to use, name:the set of color
wordcloud(wordDf$word # word
, wordDf$freq # frequency
, min.freq = 5 # minimum frequency
, colors = pal # palette information
, rot.per = 0.5 # word rotation degree
, random.order = F # decision of word appearance in random way (False or True)
, scale = c(3,1) # the front number should be large so the most freqeuntly appeared number can show up in greater size
, family="AppleGothic") # Macbook users need to set font
install.packages("treemap")
library(treemap)
treemap(wordDf # set data
,title = "Word Tree Map"
,index = c("word") # set variable that will go inside the box
,vSize = "freq"  # set box size
,fontfamily.labels = "AppleGothic" # Macbook users need to set font
,fontsize.labels = 12 # set font size
,palette=pal # palette information
,border.col = "white") # set border color
install.packages("tm") #Package for text mining
install.packages("slam")
install.packages("dplyr")
install.packages("readr") #Package to read file
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(NLP4kec)
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
#Create Corpus
corp = VCorpus(VectorSource(parsedData))
#Remove Puncutuation and Special Characters
corp = tm_map(corp, removePunctuation)
#Remove Numbers
corp = tm_map(corp, removeNumbers)
#Chnage to lower case letters
corp = tm_map(corp, tolower)
#Remove particular words
corp = tm_map(corp, removeWords, c("is", "the","or","and","for","that","this","more","so", "be"))
#manage synonyms
for (j in seq(corp))
{
corp[[j]] <- gsub("like", "love", corp[[j]])
corp[[j]] <- gsub("hate", "dislike", corp[[j]])
}
##################################################################
#change to text document form
corp = tm_map(corp, PlainTextDocument)
#create Document Term Matrix (DTM) - (length of the word is set as 2)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
#create Term Document Matirx
tdm = TermDocumentMatrix(corp, control=list(removeNumbers=TRUE, wordLengths=c(2,Inf)))
#remove Sparse Terms
dtm = removeSparseTerms(dtm, as.numeric(0.98))
#find word frequency
freq = colSums(as.matrix(dtm))
head(freq)
#save DTM as frame form
dtm_df = as.data.frame(as.matrix(dtm))
#extract DTM as CSV
write_excel_csv(dtm_df, "./dtm.csv")
#word frequency
length(freq)
#sort words in descending order
freq[head(order(-freq), 5)]
#sort words in ascending order
freq[head(order(freq), 10)]
#Find Frequent Terms (for example greater than 20 and lower than 341) )
findFreqTerms(dtm, lowfreq = 20, highfreq = 341)
wordDf = data.frame(word=names(freq), freq=freq)
install.packages("wordcloud")
library(wordcloud)
pal = brewer.pal(n = 12, name = "Set2") #  n:number of colors you want to use, name:the set of color
wordcloud(wordDf$word # word
, wordDf$freq # frequency
, min.freq = 5 # minimum frequency
, colors = pal # palette information
, rot.per = 0.5 # word rotation degree
, random.order = F # decision of word appearance in random way (False or True)
, scale = c(3,1) # the front number should be large so the most freqeuntly appeared number can show up in greater size
, family="AppleGothic") # Macbook users need to set font
wordcloud(wordDf$word # word
, wordDf$freq # frequency
, min.freq = 5 # minimum frequency
, colors = pal # palette information
, rot.per = 0.5 # word rotation degree
, random.order = F # decision of word appearance in random way (False or True)
, scale = c(3,1) # the front number should be large so the most freqeuntly appeared number can show up in greater size
, family="AppleGothic") # Macbook users need to set font
findAssocs(dtm, terms = "cookie", corlimit = 0.2)  #insert your keyword here. For example I have put cookie here
dtm_m = as.matrix(dtm)
dtm_m
cor_term = cor(dtm_m)
cor_term
cor_ref = cor_term[,"cookie"]
cor_ref
dtmW = DocumentTermMatrix(corp, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
dtmW
colnames(dtmW) = trimws(colnames(dtmW))
dtmW = dtmW[,nchar(colnames(dtm)) > 1]
dtmW = removeSparseTerms(dtmW, as.numeric(0.96))
dtmW
findAssocs(dtmW, "cookie", 0.01)
install.packages(c("igraph", "network", "sna", "GGally")) #패키지 한꺼번에 설치하기
install.packages("ggplot2")
library(igraph)
library(network)
library(sna)
library(ggplot2)
library(GGally)
# Create a Data of Network Map version ( This creates a matrix of correlation coefficient between words)
dtmW_m = as.matrix(dtmW)
cor_termW = cor(dtmW_m)
# Control the number of "Edge"
cor_termW[cor_termW < 0.25] = 0
# Create an object to draw a Network Map
net = network(cor_termW, directed = FALSE)
net
# Find the value of the "betweenness of network"Network의 betweenness and color yellow with the nodes that have the higher rank (10%)
net %v% "mode" <- ifelse(betweenness(net) > quantile(betweenness(net), 0.9), "big", "small")
node_color = c("small" = "grey", "big" = "gold")
# Set the value of Network edge size
set.edge.value(net, "edgeSize", cor_termW * 2)
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic") #맥 사용자는 이걸 써야한다
net = network(cor_termW, directed = FALSE)
net
net %v% "mode" <- ifelse(betweenness(net) > quantile(betweenness(net), 0.5), "big", "small")
node_color = c("small" = "grey", "big" = "gold")
set.edge.value(net, "edgeSize", cor_termW * 2)
ggnet2(net # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,color = "mode" # 노드 색상 구준 기준
,palette = node_color # 노드 색상
,size = "degree" # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,edge.size = "edgeSize" # 엣지의 굵기를 위에서 계산한 단어간 상관계수에 따라 다르게 하기
,family="AppleGothic") #맥 사용자는 이걸 써야한다
word_network = data.frame(word = rownames(cor_termW),
centrality = degree(net),
betweenness = betweenness(net),
eigenvector = evcent(net))
keyword = c("grain","baked","sample")  #This is just an example
sub_cor_term = cor_termW[,keyword]
head(sub_cor_term)
sub_cor_term = sub_cor_term[!(rownames(sub_cor_term) %in% keyword),]
head(sub_cor_term)
sub_cor_term = sub_cor_term[rowSums(sub_cor_term)>0,]
head(sub_cor_term)
net2 = network(sub_cor_term, directed = FALSE, matrix.type="bipartite")
ggnet2(net2 # 네트워크 객체
,label=TRUE # 노드에 라벨 표현 여부
,label.size = 3 # 라벨 폰트 사이즈
,edge.size = sub_cor_term[sub_cor_term>0] * 2
,size = degree(net2) # 노드의 크기를 degree cetrality값에 따라 다르게 하기
,family="AppleGothic")
install.packages("topicmodels")
install.packages("LDAvis")
install.packages("servr")
library(topicmodels)
library(LDAvis)
library(servr)
library(readr)
library(tm)
library(slam)
library(dplyr)
library(NLP4kec)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
install.packages("KoNLP")
library(KoNLP)
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
parsedData
parsedData = gsub(" ","  ",parsedData)
corp=VCorpus(VectorSource(parsedData))
corp = tm_map(corp, removePunctuation)
corp = tm_map(corp, tolower)
corp = tm_map(corp, removeWords, c("is", "the","or","and","for","that","this","more","so", "be"))
for (j in seq(corp))
{
corp[[j]] <- gsub("like", "love", corp[[j]])
corp[[j]] <- gsub("hate", "dislike", corp[[j]])
}
corp = tm_map(corp, PlainTextDocument)
dtm = DocumentTermMatrix(corp, control=list(removeNumbers=FALSE, wordLengths=c(2,Inf)))
dtm
colnames(dtm) = trimws(colnames(dtm))
dtm = dtm[,nchar(colnames(dtm)) > 1]
dtm = removeSparseTerms(dtm, as.numeric(0.997))
dtm
term_tfidf = tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm > 0))
boxplot(term_tfidf)
new_dtm = dtm[,term_tfidf >= 0.1]
new_dtm = new_dtm[row_sums(new_dtm) > 0,]
name = "OatmealCookies" #give a name
SEED = 2017
k = 10 #클러스터 개수 세팅
lda_tm = LDA(new_dtm, control=list(seed=SEED), k)
lda_tm
term_topic = terms(lda_tm, 30)
filePathName = paste0("./LDA_output/",name,"_",k,"_LDA_Result.csv")
write.table(term_topic, filePathName, sep=",", row.names=FALSE)
c_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
doc_topic = topics(lda_tm, 1)
doc_topic_df = as.data.frame(doc_topic)
doc_topic_df$rown = as.numeric(row.names(doc_topic_df))
doc_Prob = posterior(lda_tm)$topics
doc_Prob_df = as.data.frame(doc_Prob)
doc_Prob_df$maxProb = apply(doc_Prob_df, 1, max)
doc_Prob_df$rown = doc_topic_df$rown
parsedData = as.data.frame(parsedData)
parsedData$rown = as.numeric(row.names(parsedData))
id_topic = merge(doc_topic_df, doc_Prob_df, by="rown")
id_topic = merge(id_topic, parsedData, by="rown", all.y = TRUE)
id_topic = subset(id_topic,select=c("rown","doc_topic","maxProb"))
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
filePathName = paste0("./LDA_output/",name,"_",k,"_DOC","_LDA_Result.csv",sep="")
write.table(id_topic, filePathName, sep=",", row.names=FALSE)
posterior(lda_tm)$terms
phi = posterior(lda_tm)$terms %>% as.matrix
theta = posterior(lda_tm)$topics %>% as.matrix
vocab = colnames(phi)
doc_length = vector()
doc_topic_df=as.data.frame(doc_topic)
for( i in as.numeric(row.names(doc_topic_df))){
temp = corp[[i]]$content
doc_length = c(doc_length, nchar(temp[1]))
}
new_dtm_m = as.matrix(new_dtm)
freq_matrix = data.frame(ST = colnames(new_dtm_m),
Freq = colSums(new_dtm_m))
source("./Rscript/createChoyJson_v2.R")
json_lda = createNamJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA <- use this when it doesnt work
mds.method = canberraPCA
)
json_lda = createChoyJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA <- use this when it doesnt work
mds.method = canberraPCA
)
json_lda = createChoyJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA <- use this when it doesnt work
mds.method = canberraPCA
)
source("./Rscript/createChoyJson_v2.R")
json_lda = createChoyJson(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq,
#mds.method = jsPCA #canberraPCA <- use this when it doesnt work
mds.method = canberraPCA
)
serVis(json_lda, out.dir = paste("C:/apache-tomcat-8.5.11/webapps/",name,"_",k,sep=""), open.browser = FALSE) # Use this when you are using Windows
serVis(json_lda, open.browser = T) # Use this when you are using mac
install.packages("tm") #Package for text mining
install.packages("slam")
install.packages("readr") #Package to read file
library(readr)
install.packages("rJava")
library(rJava)
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
library(NLP4kec)
library(tm)
library(slam)
library(dplyr)
library(readr)
install.packages("rJava")
library(rJava)
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
Sys.setenv(JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.8.0_131.jdk/Contents/Home/jre")
library(NLP4kec)
install.packages("NLP4kec")
library(NLP4kec)
#Launch Morpheme Analyzer (use Standford Core NLP )
parsedData = text_parser(path = "./Rscript/Oatmeal_Cookies.xlsx"
,language = "en")
